{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e66b70e9",
   "metadata": {},
   "source": [
    "# Lang graph Quickstart Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc106c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from pydantic import BaseModel\n",
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "206f5bdb",
   "metadata": {},
   "outputs": [],
   "source": [

   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69cbf43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}, id='afdafe43-b1b1-4ac4-b5fb-f7a2bd66c59f'),\n",
       "  AIMessage(content='<function=get_weather>{\"city\": \"sf\"}', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 248, 'total_tokens': 259, 'completion_time': 0.014666667, 'prompt_time': 0.011652067, 'queue_time': 0.20969699900000002, 'total_time': 0.026318734}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f7bd09b454', 'finish_reason': 'stop', 'logprobs': None}, id='run--5b82a230-90a1-482b-a451-5b32aacef8f6-0', usage_metadata={'input_tokens': 248, 'output_tokens': 11, 'total_tokens': 259})]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "def get_weather(city: str) -> str:  \n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "model = init_chat_model(\n",
    "    \"llama-3.1-8b-instant\",\n",
    "    model_provider=\"groq\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[get_weather],\n",
    "    prompt=\"You are a helpful assistant. Use the tool `get_weather` to answer weather-related questions. End the conversation once the userâ€™s question is answered.\",\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af13908b",
   "metadata": {},
   "source": [
    "Static Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "346c5fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}, id='2e2fb6d7-b0d3-4ac9-826f-19ec04c557af'),\n",
       "  AIMessage(content='<function=get_weather>{\"city\": \"sf\"}', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 225, 'total_tokens': 236, 'completion_time': 0.014666667, 'prompt_time': 0.007364503, 'queue_time': 0.212876574, 'total_time': 0.02203117}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f7bd09b454', 'finish_reason': 'stop', 'logprobs': None}, id='run--5425f429-bfc3-47ef-86e9-0ddabc9ac9e1-0', usage_metadata={'input_tokens': 225, 'output_tokens': 11, 'total_tokens': 236})]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "def get_weather(city: str) -> str:  \n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "model = init_chat_model(\n",
    "    \"llama-3.1-8b-instant\",\n",
    "    model_provider=\"groq\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[get_weather],\n",
    "    # A static prompt that never changes\n",
    "    prompt=\"Never answer questions about the weather.\"\n",
    ")\n",
    "# Run the agent\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23115e9a",
   "metadata": {},
   "source": [
    "Dynamic prompting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f12ca52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}, id='9b5d90a0-e0a3-44d3-89f4-cf5cd704e41e'),\n",
       "  AIMessage(content='<function=get_weather>{\"city\": \"sf\"}', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 231, 'total_tokens': 242, 'completion_time': 0.014666667, 'prompt_time': 0.017609586, 'queue_time': 0.467000629, 'total_time': 0.032276253}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_a4265e44d5', 'finish_reason': 'stop', 'logprobs': None}, id='run--e7af5d31-41bb-4de7-925b-491d8f72f472-0', usage_metadata={'input_tokens': 231, 'output_tokens': 11, 'total_tokens': 242})]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AnyMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "\n",
    "def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]:  \n",
    "    user_name = config[\"configurable\"].get(\"user_name\")\n",
    "    system_msg = f\"You are a helpful assistant. Address the user as {user_name}.\"\n",
    "    return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n",
    "\n",
    "def get_weather(city: str) -> str:  \n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "model = init_chat_model(\n",
    "    \"llama-3.1-8b-instant\",\n",
    "    model_provider=\"groq\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[get_weather],\n",
    "    # A static prompt that never changes\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n",
    "    config={\"configurable\": {\"user_name\": \"John Smith\"}}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6231b5b8",
   "metadata": {},
   "source": [
    "Memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e41ab171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "def get_weather(city: str) -> str:  \n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "model = init_chat_model(\n",
    "    \"llama-3.1-8b-instant\",\n",
    "    model_provider=\"groq\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[get_weather],\n",
    "    checkpointer=checkpointer  \n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "sf_response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n",
    "    config  \n",
    ")\n",
    "ny_response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]},\n",
    "    config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341e36ac",
   "metadata": {},
   "source": [
    "Configure Structured Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b455d588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "def get_weather(city: str) -> str:  \n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "model = init_chat_model(\n",
    "    \"llama-3.1-8b-instant\",\n",
    "    model_provider=\"groq\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[get_weather],\n",
    "    response_format=WeatherResponse\n",
    ")\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
